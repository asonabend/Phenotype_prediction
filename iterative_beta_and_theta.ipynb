{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iterative beta and theta.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/asonabend/Phenotype_prediction/blob/master/iterative_beta_and_theta.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "HD9M9QXbMILu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "80485c97-eb3e-4cda-acbe-cebad4037ce8"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import linalg as LA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import metrics\n",
        "from scipy.special import expit\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bcprFrqwMpTw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "065d9f57-3f44-4040-f4f0-8598b37ef591"
      },
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#2a. Get the file\n",
        "downloaded = drive.CreateFile({'id':'1fupXX3KJWTbdI7MLPHKbDyEDKAXCzO7a'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('raprod2.ra.full.csv')  \n",
        "#3a. Read file as panda dataframe\n",
        "dat_pop = pd.read_csv('raprod2.ra.full.csv') \n",
        "\n",
        "#2b. Get the file\n",
        "downloaded = drive.CreateFile({'id':'1Igod8k0utRAqgK4BeEuZqj31JCURQQ5K'}) \n",
        "downloaded.GetContentFile('cui2vec_RAsubset.csv')  \n",
        "#3b. Read file as panda dataframe\n",
        "cui_subset = pd.read_csv('cui2vec_RAsubset.csv') \n",
        "\n",
        "#2c. Get the file\n",
        "downloaded = drive.CreateFile({'id':'1Z8YK2sVwKeA0OsZQV-sMNeFSkAI0zX02'}) \n",
        "downloaded.GetContentFile('C1858558.csv')  \n",
        "#3c. Read file as panda dataframe\n",
        "C1858558 = pd.read_csv('C1858558.csv') \n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "B1LzFEXs-uXa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "7e3fb6b9-0324-4a0d-c520-dd60786308ce"
      },
      "cell_type": "code",
      "source": [
        "# Convert to matrix and remove unnamed column:\n",
        "np_C1858558 = C1858558.loc[:, ~C1858558.columns.str.contains('^Unnamed')].as_matrix()\n",
        "\n",
        "np_cui_subset = cui_subset.loc[:, ~cui_subset.columns.str.contains('^Unnamed')].as_matrix()\n",
        "\n",
        "# Select top 10 percent:\n",
        "# Calculates cosine similarities:\n",
        "np_cos_sim = ([np_C1858558.dot(np_cui_subset[i,:])/(LA.norm(np_cui_subset[i,:])) for i in range(np_cui_subset.shape[0])]/LA.norm(np_C1858558))\n",
        "np_cos_sim = np_cos_sim/sum(np_cos_sim)\n",
        "close_CUIs = (abs(np_cos_sim) > np.percentile(abs(np_cos_sim),90)).flatten()\n",
        "np_cui_subset = np_cui_subset[close_CUIs,:]\n",
        "\n",
        "# Create tensorflow constants for RA CUI \n",
        "#tf_C1858558 = tf.constant(np_C1858558,name='RAcui', dtype=tf.float32)\n",
        "\n",
        "\n",
        "# Calculates cosine similarities:\n",
        "np_cos_sim = ([np_C1858558.dot(np_cui_subset[i,:])/(LA.norm(np_cui_subset[i,:])) for i in range(np_cui_subset.shape[0])]/LA.norm(np_C1858558))\n",
        "np_cos_sim = np_cos_sim/sum(np_cos_sim)\n",
        "\n",
        "### First I filter the non-labeled rows then I filter the CUI's not mentioned in the notes\n",
        "counts = dat_pop.loc[dat_pop['label'].isin(['Y','N'])].loc[:,list(cui_subset.iloc[close_CUIs,0])].as_matrix()\n",
        "# Patients response::\n",
        "y = dat_pop.loc[dat_pop['label'].isin(['Y','N'])].loc[:,'label'].eq('Y').mul(1).as_matrix()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U5aJjqbD_ZHL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "b76b03a4-69c1-472a-cd27-b26f6e57712c"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 140,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5PA2vDXKeKu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "9d1f162b-9132-4512-cba4-10aaed6ef522"
      },
      "cell_type": "code",
      "source": [
        "params = it_reg(y=y,counts=counts,vec_weights=np_cos_sim,cui_subset=np_cui_subset,tot_iters=50000)#,reg_type='Ridge',tot_iters=1500"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step #1 beta = [0.95252275] beta0 = [[-0.6735442]] theta = [-0.85607547 -0.18812488 -0.586894  ]\n",
            "Loss = [1.4217508]\n",
            "training AUC = 0.8291536050156739\n",
            "validation AUC = 0.8569860279441117\n",
            "\n",
            "\n",
            "Step #500 beta = [0.663204] beta0 = [[-1.719471]] theta = [-0.5369508  -0.12664568 -0.39218545]\n",
            "Loss = [0.87254226]\n",
            "training AUC = 0.6457680250783698\n",
            "validation AUC = 0.8440724604448521\n",
            "\n",
            "\n",
            "Step #1000 beta = [0.5337127] beta0 = [[-1.8077306]] theta = [-0.37962437 -0.08801726 -0.2675295 ]\n",
            "Loss = [0.6684168]\n",
            "training AUC = 0.7523510971786833\n",
            "validation AUC = 0.8816833751044278\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gNVlMzhup4LF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f56b5a34-7eda-4c69-d60e-a32303cf8c45"
      },
      "cell_type": "code",
      "source": [
        "### Define validation sample of size val_perc*patients_No ###\n",
        "val_index = np.random.choice(len(y),replace=False, size=round(.15*len(y)))\n",
        "val_y = y[val_index]\n",
        "val_counts = counts[val_index,:]\n",
        "  \n",
        "### Define training sample with the rest to the observations ###\n",
        "y = np.delete(y, val_index, 0)\n",
        "counts = np.delete(counts, val_index, 0)\n",
        "\n",
        "y_val_preds = predict(counts=val_counts,vec_weights=np_cos_sim,cui_subset=np_cui_subset,theta=params['theta'],beta=params['beta'],beta0=params['beta0'])\n",
        "metrics.roc_auc_score(val_y, y_val_preds)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8308080808080809"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "RWCBkE7w-Tse",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### Iterative regression function ###\n",
        "#####################################\n",
        "# Inputs:\n",
        "## y = binary vector containing disease status, type: numpy.ndarray\n",
        "## counts = matrix with patients as rows and counts of the j^th column mentioned in the i^th patient note at (i,j), type: numpy.ndarray\n",
        "## vec_weights = normalized weight vector of size CUIs_No (number of CUIs to be used), type: numpy.ndarray\n",
        "## cui_subset = CUI-embedding matrix with CUIs as rows and dimensions as columns, type: numpy.ndarray\n",
        "## OPTIONAL PARAMETERS:\n",
        "## val_perc = number between (0,1) to determine the percentage of sample for validation, type: \n",
        "## reg_type = Select appropriate loss function based on regression type: either Ridge or LASSO\n",
        "## tot_iters = Total iterations for the regression paramaters loop\n",
        "def it_reg(y,counts=counts,\n",
        "           vec_weights=np_cos_sim,\n",
        "           cui_subset=np_cui_subset,\n",
        "           val_perc = 0.15,        \n",
        "           reg_type='Ridge',\n",
        "           tot_iters=1500):\n",
        "  \n",
        "### Define validation sample of size val_perc*patients_No ###\n",
        "  val_index = np.random.choice(len(y),replace=False, size=round(val_perc*len(y)))\n",
        "  val_y = y[val_index]\n",
        "  val_counts = counts[val_index,:]\n",
        "  \n",
        "### Define training sample with the rest to the observations ###\n",
        "  y = np.delete(y, val_index, 0)\n",
        "  counts = np.delete(counts, val_index, 0)\n",
        "  \n",
        "  # Clear out old graph\n",
        "  tf.reset_default_graph()\n",
        "  # Create graph\n",
        "  sess = tf.Session()\n",
        "  \n",
        "### Data manipulation ###\n",
        "  \n",
        "  # Create tensorflow constant for the CUI matrix\n",
        "  tf_cui_subset = tf.constant(cui_subset,name='CUIsubset', dtype=tf.float32)\n",
        "\n",
        "  # Create tensorflow constants the cosine similarities\n",
        "  tf_cos_sim = tf.constant(vec_weights,name='cos_sim', dtype=tf.float32)\n",
        "  # X matrix which contains X_ij, the count of CUI j in patient's i notes:\n",
        "  X = counts.transpose()# Not centered anymore since I use log instead of x^2 (counts-counts.mean()).as_matrix().transpose()\n",
        "  # Count transformations, currently using x, log(x+1) and sqrt(x) # check to see if a NN to estimate optimal tranformation improves result or it's too unstable\n",
        "  np_X_poly = np.array([[X[cui,:],np.log(X[cui,:]+1),np.sqrt(X[cui,:])] for cui in range(X.shape[0])])#np.array([[np.ones(X.shape[1]),X[cui,:],np.square(X[cui,:])] for cui in range(X.shape[0])])\n",
        "  ### Now I convert it to a tensorflow constant:\n",
        "  tf_X_poly = tf.constant(np_X_poly,name='CUIcounts', dtype=tf.float32)#,shape=np_X_poly.shape\n",
        "\n",
        "  \n",
        "\n",
        "### Model Parameters ###\n",
        "\n",
        "\n",
        "  # Declare batch size\n",
        "  batch_size = counts.shape[0] ####### is this necessary now?\n",
        "\n",
        "  # Matrix Dimensions:\n",
        "  CUIs_No, CUIs_dim = cui_subset.shape\n",
        "  seed = 116687\n",
        "  patients_No = counts.shape[0]\n",
        "  theta_dim = np_X_poly.shape[1]\n",
        "  \n",
        "### Generates cosine sim. weighted CUI-embedding matrix ###\n",
        "  Vw = tf.multiply(tf_cui_subset,tf_cos_sim)\n",
        "  # Turn it into a 3D tensor for multiplications later:\n",
        "  Vw = tf.expand_dims(Vw, axis=0)\n",
        "  # Replicate the Weighted word-vec matrix for evey patient:\n",
        "  Vws = tf.tile(Vw,[patients_No,1,1])\n",
        "\n",
        "### Initializes TensorFlow parameters ###\n",
        "  ### theta such that f_j(x_ij)=theta0_j+theta1_j*X_ij+theta2_j*(X_ij)^2:\n",
        "  theta = tf.Variable(tf.random_normal(shape=[CUIs_No,1,theta_dim]),name='theta')\n",
        "  ### beta for regression:\n",
        "  beta = tf.Variable(tf.random_normal(shape=[CUIs_dim,1]),name='beta')\n",
        "  beta0 = tf.Variable(tf.random_normal(shape=[1,1]),name='beta0')\n",
        "  # Initialize placeholders\n",
        "  z_data = tf.placeholder(shape=[None, CUIs_dim], dtype=tf.float32)\n",
        "  y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n",
        "  beta_const = tf.placeholder(shape=[CUIs_dim,1], dtype=tf.float32,name='beta_const')\n",
        "  beta0_const = tf.placeholder(shape=[1,1], dtype=tf.float32,name='beta0_const')\n",
        "  \n",
        "### Declares model operations ####\n",
        "## Calculate f_j(x)=theta0j*x_ij+theta1j*log(x_ij+1)+theta2j*sqrt(x_ij) where j is a CUI\n",
        "  f_x = tf.transpose(tf.matmul(theta,tf_X_poly)) # tensor dimensions: (patiens_No,1,CUI_No)\n",
        "## Cosine sim. weighted CUI-embedding matrix:\n",
        "## Patient's feature vectors:\n",
        "  tf_z = tf.squeeze(tf.matmul(f_x,Vws))\n",
        "  model_output_beta = tf.sigmoid(tf.add(tf.matmul(z_data, beta), beta0))\n",
        "  model_output_theta = tf.sigmoid(tf.add(tf.matmul(tf_z, beta_const), beta0_const))\n",
        "  \n",
        "### Loss Functions ###\n",
        "\n",
        "# Loss functions to optimize over beta: ### Lasso is not implemented yet\n",
        "#regression_type = 'LASSO'\n",
        "  regression_type = 'Ridge'\n",
        "\n",
        "# Select appropriate loss function based on regression type\n",
        "  if reg_type == 'LASSO':\n",
        "\n",
        "    lasso_param = tf.constant(0.9)\n",
        "    heavyside_step = tf.truediv(1., tf.add(1., tf.exp(tf.multiply(-50., tf.subtract(beta, lasso_param)))))\n",
        "    regularization_param = tf.multiply(heavyside_step, 99.)\n",
        "    loss_beta = tf.add(tf.reduce_mean(tf.square(y_target - model_output_beta)), regularization_param)\n",
        "\n",
        "  elif reg_type == 'Ridge':\n",
        "# Declare the Ridge loss function\n",
        "## Ridge loss = L2_loss + L2 norm of slope\n",
        "    ridge_param = tf.constant(1.)\n",
        "    ridge_loss = tf.reduce_mean(tf.square(beta))\n",
        "    loss_beta = tf.expand_dims(tf.add(tf.reduce_mean(tf.square(y_target - model_output_beta)), tf.multiply(ridge_param, ridge_loss)), 0)\n",
        "  \n",
        "# Loss functions to optimize over theta\n",
        "\n",
        "## Regularization for theta:\n",
        "  theta_param = tf.constant(1.)\n",
        "  theta_reg_loss = tf.reduce_mean(tf.square(theta))\n",
        "  loss_theta = tf.expand_dims(tf.add(tf.reduce_mean(tf.square(y_target - model_output_theta)), tf.multiply(theta_param, theta_reg_loss)), 0)\n",
        "  \n",
        "\n",
        "### Optimizers ###\n",
        "\n",
        "# Declare optimizers\n",
        "  my_opt_beta = tf.train.GradientDescentOptimizer(1e-1)\n",
        "  train_step_beta = my_opt_beta.minimize(loss_beta)\n",
        "\n",
        "  #tf.train.AdamOptimizer(1e-4).minimize(cross_entropy) ### might be better than SGD\n",
        "  my_opt_theta = tf.train.GradientDescentOptimizer(1e-1)\n",
        "  train_step_theta = my_opt_theta.minimize(loss_theta)\n",
        "\n",
        "### Runs regression: ###\n",
        "# make results reproducible\n",
        "  np.random.seed(seed)\n",
        "  tf.set_random_seed(seed)\n",
        "\n",
        "# Initialize variables\n",
        "  init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
        "  sess.run(init)\n",
        "  np_Z = sess.run(tf_z)\n",
        "# Training loop\n",
        "  loss_vec, trainAUC_vec, valAUC_vec = [],[],[]\n",
        "  for i in range(tot_iters):\n",
        "      rand_index = np.random.choice(np_Z.shape[0], size=batch_size)\n",
        "      rand_z = np_Z[rand_index,:]\n",
        "      rand_y = np.array([y[rand_index]]).transpose()\n",
        "      sess.run(train_step_beta, feed_dict={z_data: rand_z, y_target: rand_y})\n",
        "      ##\n",
        "      np_beta = sess.run(beta)\n",
        "      np_beta0 = sess.run(beta0)\n",
        "      y_pred = expit(rand_z.dot(np_beta)+np_beta0)\n",
        "      sess.run(train_step_theta, feed_dict={beta_const: np_beta, beta0_const: np_beta0, y_target: rand_y})\n",
        "      rand_z = sess.run(tf_z)\n",
        "      ##\n",
        "      temp_loss = sess.run(loss_beta, feed_dict={z_data: rand_z, y_target: rand_y})\n",
        "      val_y_preds = predict(counts=val_counts,vec_weights=vec_weights,cui_subset=cui_subset,theta=sess.run(theta),beta=np_beta,beta0=np_beta0)\n",
        "      temp_valAUC = metrics.roc_auc_score(val_y, val_y_preds)\n",
        "      temp_trainAUC = metrics.roc_auc_score(rand_y, y_pred)\n",
        "      loss_vec.append(temp_loss[0])\n",
        "      valAUC_vec.append(temp_valAUC)\n",
        "      trainAUC_vec.append(temp_trainAUC)\n",
        "      \n",
        "      if (i+1)%500==0 or i == 0:\n",
        "        print('Step #' + str(i+1) + ' beta = ' + str(sess.run(beta)[0]) + ' beta0 = ' + str(sess.run(beta0)) + ' theta = ' + str(sess.run(theta[0,0])))\n",
        "        print('Loss = ' + str(temp_loss))\n",
        "        print('training AUC = ' + str(temp_valAUC))\n",
        "        print('validation AUC = ' + str(temp_trainAUC))\n",
        "        print('\\n')\n",
        " \n",
        "\n",
        "### Extract regression results: ###\n",
        "\n",
        "  results = {'beta0': np_beta0, 'beta': np_beta, 'theta': sess.run(theta),'auc': metrics.roc_auc_score(rand_y, y_pred),\n",
        "             'Loss': loss_vec, 'valAUC':valAUC_vec, 'trainAUC':trainAUC_vec}\n",
        "  \n",
        "### Plots loss over time: ###\n",
        "  plt.plot(loss_vec, 'k-')\n",
        "  plt.title('regression_type' + ' Loss per Generation')\n",
        "  plt.xlabel('Generation')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.show()\n",
        "  return(results)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Hy9Pc10-R40",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Computes model outcome:\n",
        "# Inputs:\n",
        "## counts = matrix with patients as rows and counts of the j^th column mentioned in the i^th patient note at (i,j), type: numpy.ndarray\n",
        "## vec_weights = normalized weight vector of size CUIs_No (number of CUIs to be used), type: numpy.ndarray\n",
        "## cui_subset = CUI-embedding matrix with CUIs as rows and dimensions as columns, type: numpy.ndarray\n",
        "## theta = matrix with theta parameters for f_j(x)=theta0j*x_ij+theta1j*log(x_ij+1)+theta2j*sqrt(x_ij) where j is a CUI, type: numpy.ndarray\n",
        "## beta, beta0 = coefficients for regression, type: numpy.ndarray\n",
        "def predict(counts,\n",
        "           vec_weights,\n",
        "           cui_subset,\n",
        "           theta,        \n",
        "           beta,\n",
        "           beta0):\n",
        "\n",
        "  # Clear out old graph\n",
        "  tf.reset_default_graph()\n",
        "  # Create graph\n",
        "  sess = tf.Session()\n",
        "  \n",
        "### Data manipulation ###\n",
        "  \n",
        "  # Create tensorflow constant for the CUI matrix\n",
        "  tf_cui_subset = tf.constant(cui_subset,name='CUIsubset', dtype=tf.float32)\n",
        "\n",
        "  # Create tensorflow constants the cosine similarities\n",
        "  tf_cos_sim = tf.constant(vec_weights,name='cos_sim', dtype=tf.float32)\n",
        "  # X matrix which contains X_ij, the count of CUI j in patient's i notes:\n",
        "  X = counts.transpose()\n",
        "  # Count transformations, currently using x, log(x+1) and sqrt(x) \n",
        "  np_X_poly = np.array([[X[cui,:],np.log(X[cui,:]+1),np.sqrt(X[cui,:])] for cui in range(X.shape[0])])\n",
        "  ### Now I convert it to a tensorflow constant:\n",
        "  tf_X_poly = tf.constant(np_X_poly,name='CUIcounts', dtype=tf.float32)#,shape=np_X_poly.shape\n",
        "\n",
        "### Initializes TensorFlow constants for parameters ###\n",
        "  theta = tf.constant(theta,name='theta', dtype=tf.float32)\n",
        "  beta0 = tf.constant(beta0,name='beta0', dtype=tf.float32)\n",
        "  beta = tf.constant(beta,name='beta', dtype=tf.float32)\n",
        "\n",
        "### Model dimensions ###\n",
        "\n",
        "  # Matrix Dimensions:\n",
        "  CUIs_No, CUIs_dim = cui_subset.shape\n",
        "  patients_No = counts.shape[0]\n",
        "  theta_dim = np_X_poly.shape[1]\n",
        "  \n",
        "### Generates cosine sim. weighted CUI-embedding matrix ###\n",
        "  Vw = tf.multiply(tf_cui_subset,tf_cos_sim)\n",
        "  # Turn it into a 3D tensor for multiplications later:\n",
        "  Vw = tf.expand_dims(Vw, axis=0)\n",
        "  # Replicate the Weighted word-vec matrix for evey patient:\n",
        "  Vws = tf.tile(Vw,[patients_No,1,1])\n",
        "\n",
        "### Declares model operations ####\n",
        "  # Calculate f_j(x)=theta0j*x_ij+theta1j*log(x_ij+1)+theta2j*sqrt(x_ij) where j is a CUI\n",
        "  f_x = tf.transpose(tf.matmul(theta,tf_X_poly)) # tensor dimensions: (patiens_No,1,CUI_No)\n",
        "  # Cosine sim. weighted CUI-embedding matrix:\n",
        "  # Patient's feature vectors:\n",
        "  tf_z = tf.squeeze(tf.matmul(f_x,Vws))\n",
        "  model_output = tf.sigmoid(tf.add(tf.matmul(tf_z, beta), beta0))\n",
        "  \n",
        "  ### Extract output results: ###\n",
        "\n",
        "  results = sess.run(model_output)\n",
        "  return(results)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pof9sPiAqIi9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}